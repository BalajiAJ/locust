https://betterprogramming.pub/introduction-to-locust-an-open-source-load-testing-tool-in-python-2b2e89ea1ff

https://stackoverflow.com/questions/51131278/locust-load-testing-appropiate-number-of-users-to-simulate-and-hatch-rate

Related Work

Tokenization Strategies for Training Models on Code: Tokenization is a way of separating a piece of text into smaller units called tokens. Typically Natural Language tokenizers are adopted for tokenizing Programming Languages. These tokenizers are not effective as the distribution of words in code is different from natural text. This session will detail tokenization strategies for Programming Languages by covering tokenizations in OpenAI Codex and Facebook TransCoder.

PL Tokenizers

Facebook Transcoder (https://arxiv.org/pdf/2006.03511.pdf): Recent approaches in multilingual natural language processing tend to use a common tokenizer [28], and a shared vocabulary for all languages. This reduces the overall vocabulary size, and maximizes the token overlap between languages, improving the cross-linguality of the model [14, 29]. In our case, a universal tokenizer would be suboptimal, as different languages use different patterns and keywords. The logical operators && and || exist in C++ where they should be tokenized as a single token, but not in Python. The indentations are critical in Python as they define the code structure, but have no meaning in languages like C++ or Java. We use the javalang5 tokenizer for Java, the tokenizer of the standard library for Python6, and the clang7 tokenizer for C++. These tokenizers ensure that meaningless modifications in the code (e.g. adding extra new lines or spaces) do not have any impact on the tokenized sequence. An example of tokenized code is given in Figure 3 in the appendix. We learn BPE codes [42] on extracted tokens, and split tokens into subword units. The BPE codes are learned with fastBPE8 on the concatenation of tokenized C++, Java, and Python files.

OpenAI Codex (https://arxiv.org/pdf/2107.03374.pdf): In order to maximally leverage text representations from GPT, we base our code lexer on the GPT-3 text tokenizer. Since the distribution of words in GitHub code differs from that of natural text, this tokenizer is not very effective for representing code. The largest source of inefficiency arises from encoding whitespace, so we add an additional set of tokens for representing whitespace runs of different lengths. This allows us to represent code using approximately 30% fewer tokens.

GPT-Neo-125M-Code-Clippy (https://huggingface.co/flax-community/gpt-neo-125M-code-clippy): As discussed in OpenAI's Codex paper, we modified the GPT-Neo model and tokenizer to accommodate for additional whitespace characters. Specifically, we add the following tokens ["\t\t", "  ", "    ", "        "] and since they are all related to indentation, we initialize the embedding layer of these tokens with the same weights as the \t token already present in the model in hopes the model will learn to associate these whitespace characters with indentation faster. A script to automatically do this can be found here.

Salesforce CodeT5 (https://arxiv.org/pdf/2109.00859.pdf): In addition, we propose to leverage the developer-assigned identifiers in code. When writing programs, developers tend to employ informative identifiers to make the code more understandable, so that these identifiers would generally preserve rich code semantics, e.g., the “binarySearch” identifier in Figure 2 directly indicates its functionality. To fuse such code-specific knowledge, we propose a novel identifier-aware objective that trains the model to distinguish which tokens are identifiers and recover them when they are masked….Moreover, we employ the whole word masking by sampling spans before subword tokenization, which aims to avoid masking partial subtokens and is shown to be helpful (Sun et al., 2019)….Tokenization is a key ingredient for the success of pre-trained language models like BERT and GPT. They often employ a Byte-Pair Encoding (BPE) tokenizer (Sennrich et al., 2016) to alleviate the Out-of-Vocabulary (OoV) issues. Specifically, we train a Byte-level BPE tokenizer following Radford et al. (2019) and set the vocabulary size to 32,000 as T5.

We add additional special tokens ([PAD], [CLS], [SEP], [MASK0], ..., [MASK99]). This tokenzier is trained on all of our pre-training data with non-printable characters and low-frequent tokens (occurring <3 times) filtered. We compare it with T5’s default tokenizer and find that our tokenizer largely reduces the length of tokenized code sequence by 30% - 45% on downstream tasks. This will accelerate the training and especially benefit generation tasks due to the shorter sequence to predict. We also spot a severe problem for applying the T5’s default tokenizer on source code, where it would encode some common code tokens such as brackets [‘{’, ‘}’] into unknown tokens.

PaLM was trained using a combination of English and multilingual datasets that include high-quality web documents, books, Wikipedia, conversations, and GitHub code. We also created a “lossless” vocabulary that preserves all whitespace (especially important for code), splits out-of-vocabulary Unicode characters into bytes, and splits numbers into individual tokens, one for each digit. ( https://ai.googleblog.com/2022/04/pathways-language-model-palm-scaling-to.html)

Techniques for improving tokenization efficiency

How data records are pre-processed to attain behavior similar to pre-trained tokenizer for newly added keywords.
1. Space encoding (+tabs, +new lines).
2. Keyword encoding (+operators). Co-occurrence?
3. Case encoding.
4. Non-English character encoding. (https://arxiv.org/pdf/2003.07914.pdf)
5. (a) Replace Identifiers with smaller alphabets (lesser tokens). (b) Effect of Identifier Tokenization on Automatic Source Code Documentation (https://doi.org/10.1007/s13369-021-06149-7).
6. Remove comments: License Comments, Task annotations, Long comments, Commented source code, Automatically generated comments by the IDE. https://ieeexplore.ieee.org/document/7820211: Using Natural Language Processing to Automatically Detect Self-Admitted Technical Debt.
7. Encode string literals.
8. Minification of JavaScript, https://github.com/dflook/python-minifier for Python, ..
9. Learned Token Pruning for Transformers (https://arxiv.org/pdf/2107.00910.pdf), Dynamic token reduction for accelerating BERT inference (it is using RL and so it might not be useful) (https://arxiv.org/pdf/2105.11618.pdf).
10. Code tokens and sub token encodings for source code (https://arxiv.org/pdf/2004.13651.pdf).
11. Import statements can be removed as it does not add value to documentation.
12. Towards Automatically Generating Summary Comments for Java Methods (https://dl.acm.org/doi/abs/10.1145/1858996.1859006): Use this model to remove irrelevant sections of code with respect to documentation and then pass the shortened code to ML model for documentation.
13. ? (How to improve (reduce) Fertility by adopting morphology in NL and adopt it for PL?).

Evaluation of Tokenization Schemes

Here is the list of the standard metrics for evaluating different tokenization schemes (for non-tokenization-free models) without training a full-sized model.

1. Sub-Word Fertility / Sequence Lengths: Sub-Word Fertility is the average number of sub words produced per tokenized word (http://juditacs.github.io/2019/02/19/bert-tokenization-stats.html). Sequence Lengths: Increase in length of the encoded sentence increases the computational cost for sequence modeling (https://arxiv.org/pdf/2004.14870.pdf). [@samson] I believe fertility and sequence length are two operationalizations of the same idea, one is a word level metric and the other is a sentence level one. [@Mike Tian-Jian Jiang] Word level metrics would be less reliable on non whitespaced languages (From characters to words: the turning point of BPE merges - ACL Anthology (aclweb.org)).

2. Proportion of Continued Words: Proportion of words in the corpus where the tokenized word is continued across at least two sub-tokens (denoted by continuation symbols ##) (http://juditacs.github.io/2019/02/19/bert-tokenization-stats.html).

3. UNK ratio / Under-trained Tokens: High OOV rates are highly likely to degrade the performance (https://arxiv.org/pdf/2010.02534.pdf).

4. Vocabulary Coverage: Coverage of a representative corpus by a vocabulary’s symbols, measured by computing the total number of tokens (words and punctuation) in the corpus that are represented in the vocabulary divided by the total number of tokens in the corpus (https://arxiv.org/pdf/2004.14870.pdf).

5. Symbol Complexity: Total number of symbols needed to encode a representative set of word types (https://arxiv.org/pdf/2004.14870.pdf).

6. Adaptability:  [@Roman CASTAGNE] Computing these tokenizer metrics on out-of-domain dataset and evaluating how much our tokenizer would suffer from this shift. 

7. Inference Speed: Time required to infer predictions (2105.13626.pdf (arxiv.org)).

8. [Multi-lingual metric] % of monolingual_vocab in multilingual_vocab: Our simple ‘fair baseline’ could consist in training monolingual tokenisers on each language, with equal vocabulary sizes, and then use the union of all resulting vocabularies to create a multilingual tokeniser. Going forward, we might deviate from generating/selecting equal mono-lingual vocabulary sizes. Then, higher the %_monolingual_vocab in %_multilingual_vocab, the better the performance (https://arxiv.org/abs/2012.15613).

9. [Multi-lingual metric] multilingual_vocab vs UD_treebanks_tokens for each language: Compares the tokenizations produced by the monolingual models / multilingual models with the reference tokenizations provided by the human dataset annotators with respect to their sentence lengths (https://arxiv.org/abs/2012.15613).

10. LinMaxMatch: https://ai.googleblog.com/2021/12/a-fast-wordpiece-tokenization-system.html.



[2:25 PM] Kamalkumar Rathinasamy

Spaces and Keywords: We have seen that text-based tokenizers cannot represent code efficiently. Whitespaces and tabs form the largest source of inefficiency. We add additional set of tokens to the tokenizer vocabulary for varying lengths of spaces and tabs to tackle this inefficiency. We also observed that the vocabulary of CodeBERT was missing some Python keywords, like ‘elif’ and ‘nonlocal’ and we also added these keywords as tokens to the tokenizer vocabulary. As a result, these keywords were being efficiently tokenized.
Space Encoding: We had tab tokens, where each tab is equivalent of 4 spaces, to the pre-trained codeBERT tokenizer, which in turn reduces the number of tokens for the input. This is an important step in reducing the number of tokens in space sensitive programming languages like Python.
Keyword Encoding: CodeBERT tokenizer already has the programming language keywords considered during their pre-training. But there were couple of Python keywords such as nonlocal and elif which was not part of the same, hence we add the same to the tokenizer to bring in reduction of tokens.



There are different minification options that we use. These options are as mentioned below:

1. Removing literal statements: Removes statements that entirely consist of a literal value (including docstrings) or replaces with literal zero, if a statement is required.

2. Removing annotations: Removes function and variable annotations.

3. Renaming locals: Shortens any non-global names including local variables, local imports, functions and classes in function scope and so on.

4. Renaming globals: Shortens names in the module scope, including introducing short names for built-ins.

5. Hoist literals: Replaces string and bytes literals with references to module level variables and may introduce new names for some built-in constants.

6. Convert posargs to args: Converts positional only arguments into normal arguments by removing the ‘/’ separator in the argument list.

7. Remove object base: Removes ‘object’ from base class list of all classes in Python 3.

8. Remove pass: Removes pass statements or replaces with literal zero, if a statement is required.

9. Combine imports: Combines adjacent import statements into one without changing order of the imports

Examples for each of these options are presented  in Table 1.




4.1.1 Settings
We used CodeSearchNet dataset for our experiments since it was used to train the CodeBERT [3] model. This dataset has Python code samples and its equivalent documentation. The experiment follows the following steps:
1. First the raw data i.e. Python code is fed to the tokenizer and tokenization metrics are generated. This serves as the base for all further experiments.
2. Next, we perform space encoding (details below) and then pre-process the data using Python minifier. Python minifier transforms Python code into its most compact representation. There are advanced minification options available as well with the Python minifier which minify the data using various techniques like combining adjacent import statements into a single statement, removing pass statements, removing annotations and so on. In this step, we use the Python minifier in its raw form and do not use any of the advanced options. The tokenization metrics is recalculated and recorded.
3. Now, we add keyword encoding (details below) to the previous step and repeat the process. The tokenization metrics is recalculated and recorded again.
4. Next, we chose to use the advanced minification options of the Python minifier. We first perform space and keyword encoding and then select one advanced minification option to recalculate the tokenization metrics and record them. We repeat this step by cumulatively including more options, one at a time.



Tokenizers specific to programming languages were used to train the Facebook Transcoder model. Recent multilingual natural language processing algorithms use a common tokenizer and a shared vocabulary for all languages. This reduces the overall vocabulary size while increasing token overlap between languages, enhancing the model's cross-linguality. A universal tokenizer would be ineffective for training a multilingual model because different languages use different patterns and keywords. As a result, language-specific tokenizers were employed for Transcoder, such as the javalang5 tokenizer for Java, the standard library tokenizer for Python6, and the clang7 tokenizer for C++. These tokenizers ensured that insignificant code changes (such as adding extra new lines or spaces) had no effect on the tokenized sequence.

Similarly, the code lexer for Open AI's Codex model was constructed on top of the GPT-3 text tokenizer to get the most out of GPT text representations. This tokenizer was not particularly effective for expressing code since the distribution of words in GitHub code varied from that of normal text. Because encoding whitespace causes the most inefficiency, they introduced an additional set of tokens to represent whitespace runs of various lengths. As a result, code was represented with around 30% fewer tokens.

GPT-Neo-125M-Code-Clippy model’s tokenizer was changed to accommodate additional whitespace characters, based on the idea of Open AI’s Codex tokenizer. Specifically, these tokens ["\t\t", "  ", "    ", "        "] were added and because they are all connected to indentation, the embedding layer of these tokens were initialized with the same weights as the \t token already existing in the model. This was done to help the model identify these whitespace characters with indentation more quickly.

The tokenizer in the Salesforce CodeT5 model leveraged the developer-assigned identifiers in code.  When writing program, developers frequently use informative identifiers to make the code more comprehensible, with the expectation that these identifiers will keep rich code semantics. A unique identifier-aware aim has been developed to determine which tokens are identifiers and recover them when they are masked in order to fuse such code-specific information.  Before subword tokenization, sampling spans use full word masking to avoid masking partial subtokens, which has been proved to be useful (Sun et al., 2019)

Special tokens ([PAD], [CLS], [SEP], [MASK0], ..., [MASK99]) were also added to the tokenizer. Non-printable characters and tokens that occurred less than three times were filtered out of the tokenizer's training data.  On downstream tasks, the tokenizer reduces the length of the tokenized code sequence by 30 percent to 45 percent when compared to T5's default tokenizer. Due to the shorter sequence to predict, this technique helped to speed up the training and benefit generation tasks. To solve the difficulty of encoding common code tokens such brackets [", "] into unknown tokens, an additional set of tokens was introduced to the tokenizer.

PaLM's tokenizer was trained on 'lossless' vocabulary, which maintains all whitespace (particularly crucial for code), splits Unicode characters into bytes, and breaks integers into distinct tokens, one for each digit.

As an input sequence goes through transformer layers, Learned Token Pruning (LTP) was developed to adaptively delete unnecessary tokens. LTP prunes tokens with an attention score less than a threshold value learned after training for each layer.

Joint Optimization of Tokenization and Downstream Model proposes to find appropriate tokenization for a given downstream task by jointly optimizing a tokenizer and the model. The method comprises of steps such as Optimizing Tokenization with Loss, employing a neural unigram language model (NULM) as the tokenizer. The loss value for the tokenizer during training are computed for multiple tokenization and the one which has least loss is considered for the given downstream task. 

In programming languages, source code identifiers are often made up of smaller parts. For example, list_organization_employee identifier is made up of three subtokens (list, organization, employee). SUBTOKEN learns how to combine the meaning of an identifier's subtokens into a single encoding. This provides for a better representation of the sparse nature of identifiers while also lowering memory needs.

